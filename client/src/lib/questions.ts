// Questions data converted from the original CSV
export interface Question {
  id: number;
  question: string;
  option_a: string;
  option_b: string;
  option_c: string;
  option_d: string;
  correct_answer: string;
  difficulty: number;
  concept: string;
  explanation: string;
  hint: string;
}

export const CONCEPTS = [
  "Supervised Learning",
  "Unsupervised Learning",
  "Optimization",
  "Overfitting",
  "Metrics",
  "Neural Networks",
  "Theory",
  "Methodology",
  "Regularization",
  "Ensemble"
] as const;

export type Concept = typeof CONCEPTS[number];

export const questions: Question[] = [
  {id:1,question:"What is the primary goal of supervised learning?",option_a:"Finding hidden patterns in data",option_b:"Learning a mapping from inputs to outputs using labeled data",option_c:"Reducing the dimensionality of data",option_d:"Generating new data samples",correct_answer:"B",difficulty:0.2,concept:"Supervised Learning",explanation:"Supervised learning uses labeled training data to learn a function that maps inputs to outputs.",hint:"Think about what 'supervised' means — someone provides the correct answers."},
  {id:2,question:"Which algorithm is commonly used for classification tasks?",option_a:"K-Means",option_b:"PCA",option_c:"Logistic Regression",option_d:"Apriori",correct_answer:"C",difficulty:0.25,concept:"Supervised Learning",explanation:"Logistic Regression is a classification algorithm that predicts categorical outcomes.",hint:"Which of these algorithms outputs a probability for class membership?"},
  {id:3,question:"What does the bias-variance tradeoff describe?",option_a:"The tradeoff between training speed and accuracy",option_b:"The tradeoff between model complexity and generalization",option_c:"The tradeoff between precision and recall",option_d:"The tradeoff between supervised and unsupervised learning",correct_answer:"B",difficulty:0.55,concept:"Theory",explanation:"The bias-variance tradeoff describes how model complexity affects underfitting (high bias) vs overfitting (high variance).",hint:"Think about what happens when a model is too simple vs too complex."},
  {id:4,question:"What is overfitting?",option_a:"When a model performs well on training data but poorly on new data",option_b:"When a model is too simple to capture patterns",option_c:"When training takes too long",option_d:"When the dataset is too small",correct_answer:"A",difficulty:0.2,concept:"Overfitting",explanation:"Overfitting occurs when a model memorizes training data instead of learning general patterns.",hint:"The model works great in practice but fails the real test."},
  {id:5,question:"Which technique helps prevent overfitting?",option_a:"Increasing model complexity",option_b:"Removing training data",option_c:"Regularization",option_d:"Using a larger learning rate",correct_answer:"C",difficulty:0.3,concept:"Regularization",explanation:"Regularization adds a penalty term to the loss function to discourage overly complex models.",hint:"This technique adds a constraint to the model's learning process."},
  {id:6,question:"What is the purpose of cross-validation?",option_a:"To increase the training dataset size",option_b:"To estimate model performance on unseen data",option_c:"To select the best features",option_d:"To speed up training",correct_answer:"B",difficulty:0.35,concept:"Methodology",explanation:"Cross-validation splits data into folds to estimate how well a model generalizes to independent data.",hint:"It's about testing your model fairly without a separate test set."},
  {id:7,question:"What does K-Means clustering do?",option_a:"Classifies data into predefined categories",option_b:"Groups similar data points into K clusters",option_c:"Reduces data dimensions",option_d:"Predicts continuous values",correct_answer:"B",difficulty:0.25,concept:"Unsupervised Learning",explanation:"K-Means partitions data into K clusters by minimizing within-cluster variance.",hint:"Think about grouping similar items together without labels."},
  {id:8,question:"What is gradient descent?",option_a:"A method to evaluate model accuracy",option_b:"An optimization algorithm to minimize a loss function",option_c:"A technique for data preprocessing",option_d:"A type of neural network",correct_answer:"B",difficulty:0.3,concept:"Optimization",explanation:"Gradient descent iteratively adjusts parameters in the direction that reduces the loss function.",hint:"Imagine walking downhill to find the lowest point."},
  {id:9,question:"What is the learning rate in gradient descent?",option_a:"The number of training epochs",option_b:"The step size for parameter updates",option_c:"The ratio of training to test data",option_d:"The model's accuracy score",correct_answer:"B",difficulty:0.35,concept:"Optimization",explanation:"The learning rate controls how large each step is when updating model parameters.",hint:"Too big and you overshoot; too small and you never get there."},
  {id:10,question:"What is precision in classification?",option_a:"The ratio of correct predictions to total predictions",option_b:"The ratio of true positives to predicted positives",option_c:"The ratio of true positives to actual positives",option_d:"The harmonic mean of sensitivity and specificity",correct_answer:"B",difficulty:0.4,concept:"Metrics",explanation:"Precision = TP / (TP + FP). It measures how many predicted positives are actually positive.",hint:"Of all the items you predicted as positive, how many were correct?"},
  {id:11,question:"What is recall (sensitivity)?",option_a:"The ratio of true positives to predicted positives",option_b:"The ratio of true negatives to actual negatives",option_c:"The ratio of true positives to actual positives",option_d:"The overall accuracy of the model",correct_answer:"C",difficulty:0.4,concept:"Metrics",explanation:"Recall = TP / (TP + FN). It measures how many actual positives were correctly identified.",hint:"Of all the actual positive cases, how many did you catch?"},
  {id:12,question:"What is the F1 score?",option_a:"The average of precision and recall",option_b:"The harmonic mean of precision and recall",option_c:"The geometric mean of precision and recall",option_d:"The maximum of precision and recall",correct_answer:"B",difficulty:0.45,concept:"Metrics",explanation:"F1 = 2 * (Precision * Recall) / (Precision + Recall). It balances precision and recall.",hint:"It's a special kind of average that penalizes extreme differences."},
  {id:13,question:"What is a neural network activation function?",option_a:"A function that initializes weights",option_b:"A function that introduces non-linearity into the network",option_c:"A function that normalizes inputs",option_d:"A function that computes the loss",correct_answer:"B",difficulty:0.35,concept:"Neural Networks",explanation:"Activation functions like ReLU and sigmoid introduce non-linearity, allowing networks to learn complex patterns.",hint:"Without this, a neural network would just be a linear model."},
  {id:14,question:"What is backpropagation?",option_a:"A method for collecting training data",option_b:"An algorithm for computing gradients in neural networks",option_c:"A technique for data augmentation",option_d:"A way to initialize network weights",correct_answer:"B",difficulty:0.45,concept:"Neural Networks",explanation:"Backpropagation computes gradients of the loss with respect to each weight using the chain rule.",hint:"It propagates error signals backward through the network."},
  {id:15,question:"What is dropout in neural networks?",option_a:"Removing entire layers from the network",option_b:"Randomly deactivating neurons during training",option_c:"Reducing the learning rate over time",option_d:"Pruning the network after training",correct_answer:"B",difficulty:0.4,concept:"Regularization",explanation:"Dropout randomly sets a fraction of neurons to zero during training to prevent co-adaptation.",hint:"Some neurons take a random 'break' during each training step."},
  {id:16,question:"What is the purpose of a loss function?",option_a:"To measure how well the model fits the training data",option_b:"To determine the learning rate",option_c:"To select features",option_d:"To split data into train and test sets",correct_answer:"A",difficulty:0.25,concept:"Optimization",explanation:"The loss function quantifies the difference between predicted and actual values.",hint:"It tells you how 'wrong' your model's predictions are."},
  {id:17,question:"What is PCA (Principal Component Analysis)?",option_a:"A supervised classification method",option_b:"A dimensionality reduction technique",option_c:"A clustering algorithm",option_d:"A regression method",correct_answer:"B",difficulty:0.4,concept:"Unsupervised Learning",explanation:"PCA finds the directions of maximum variance in data and projects it onto fewer dimensions.",hint:"It helps you see the most important patterns in high-dimensional data."},
  {id:18,question:"What is the difference between L1 and L2 regularization?",option_a:"L1 uses absolute values; L2 uses squared values of weights",option_b:"L1 is for regression; L2 is for classification",option_c:"L1 increases model complexity; L2 decreases it",option_d:"There is no difference",correct_answer:"A",difficulty:0.55,concept:"Regularization",explanation:"L1 (Lasso) adds |w| penalty promoting sparsity; L2 (Ridge) adds w² penalty promoting small weights.",hint:"One makes weights exactly zero; the other just makes them small."},
  {id:19,question:"What is ensemble learning?",option_a:"Training a single large model",option_b:"Combining multiple models to improve performance",option_c:"Using a single algorithm on multiple datasets",option_d:"Training models sequentially",correct_answer:"B",difficulty:0.35,concept:"Ensemble",explanation:"Ensemble methods combine predictions from multiple models to achieve better accuracy and robustness.",hint:"Think 'wisdom of the crowd' for machine learning."},
  {id:20,question:"What is Random Forest?",option_a:"A single decision tree with random splits",option_b:"An ensemble of decision trees trained on random subsets",option_c:"A neural network with random weights",option_d:"A clustering algorithm",correct_answer:"B",difficulty:0.35,concept:"Ensemble",explanation:"Random Forest builds many decision trees on random data subsets and aggregates their predictions.",hint:"Many trees make a forest, and randomness makes it robust."},
  {id:21,question:"What is the vanishing gradient problem?",option_a:"Gradients become too large during training",option_b:"Gradients become extremely small, slowing learning in deep networks",option_c:"The model converges too quickly",option_d:"The loss function has no gradient",correct_answer:"B",difficulty:0.6,concept:"Neural Networks",explanation:"In deep networks, gradients can shrink exponentially through layers, making early layers learn very slowly.",hint:"Signals get weaker as they travel through many layers."},
  {id:22,question:"What is batch normalization?",option_a:"Normalizing the entire dataset before training",option_b:"Normalizing layer inputs within each mini-batch during training",option_c:"Adjusting batch size during training",option_d:"A method for data augmentation",correct_answer:"B",difficulty:0.5,concept:"Neural Networks",explanation:"Batch normalization normalizes activations in each layer, stabilizing and accelerating training.",hint:"It keeps the internal signals at a healthy scale."},
  {id:23,question:"What is the ROC curve?",option_a:"A plot of precision vs recall",option_b:"A plot of true positive rate vs false positive rate",option_c:"A plot of accuracy vs model complexity",option_d:"A plot of loss vs epochs",correct_answer:"B",difficulty:0.45,concept:"Metrics",explanation:"The ROC curve shows the tradeoff between sensitivity (TPR) and specificity (1-FPR) at various thresholds.",hint:"It shows how well your model distinguishes between classes at different thresholds."},
  {id:24,question:"What is AUC (Area Under the Curve)?",option_a:"The total area of the confusion matrix",option_b:"The area under the ROC curve, measuring overall classification performance",option_c:"The area under the loss curve",option_d:"The area under the precision-recall curve only",correct_answer:"B",difficulty:0.5,concept:"Metrics",explanation:"AUC summarizes the ROC curve into a single number. AUC=1 is perfect; AUC=0.5 is random.",hint:"A single number that tells you how good your classifier is overall."},
  {id:25,question:"What is a convolutional neural network (CNN) primarily used for?",option_a:"Natural language processing",option_b:"Image recognition and processing",option_c:"Time series forecasting",option_d:"Tabular data analysis",correct_answer:"B",difficulty:0.35,concept:"Neural Networks",explanation:"CNNs use convolutional layers to automatically learn spatial features from images.",hint:"Think about processing visual information with local patterns."},
  {id:26,question:"What is transfer learning?",option_a:"Transferring data between datasets",option_b:"Using a pre-trained model as a starting point for a new task",option_c:"Moving a model from one computer to another",option_d:"Converting between model architectures",correct_answer:"B",difficulty:0.4,concept:"Methodology",explanation:"Transfer learning leverages knowledge from a model trained on one task to improve performance on a related task.",hint:"Why start from scratch when someone already learned something useful?"},
  {id:27,question:"What is the purpose of the softmax function?",option_a:"To normalize inputs to a 0-1 range",option_b:"To convert logits into probability distributions over classes",option_c:"To introduce non-linearity",option_d:"To compute the loss",correct_answer:"B",difficulty:0.45,concept:"Neural Networks",explanation:"Softmax converts raw scores (logits) into probabilities that sum to 1 across all classes.",hint:"It turns numbers into probabilities for multi-class problems."},
  {id:28,question:"What is the curse of dimensionality?",option_a:"Having too few features",option_b:"Problems that arise when working with high-dimensional data",option_c:"When the model has too many parameters",option_d:"When training takes too long",correct_answer:"B",difficulty:0.55,concept:"Theory",explanation:"As dimensions increase, data becomes sparse, distances become less meaningful, and more data is needed.",hint:"More features isn't always better — space gets very empty in high dimensions."},
  {id:29,question:"What is feature engineering?",option_a:"Automatically selecting the best algorithm",option_b:"Creating new features from existing data to improve model performance",option_c:"Removing all features except the target",option_d:"Normalizing all features to the same scale",correct_answer:"B",difficulty:0.3,concept:"Methodology",explanation:"Feature engineering involves creating, transforming, or selecting features to help models learn better.",hint:"It's about crafting the right inputs for your model."},
  {id:30,question:"What is a support vector machine (SVM)?",option_a:"A neural network variant",option_b:"A classifier that finds the optimal hyperplane to separate classes",option_c:"An unsupervised clustering method",option_d:"A dimensionality reduction technique",correct_answer:"B",difficulty:0.45,concept:"Supervised Learning",explanation:"SVM finds the hyperplane that maximizes the margin between classes in the feature space.",hint:"Think about drawing the best possible boundary between two groups."},
  {id:31,question:"What is the purpose of a validation set?",option_a:"To train the model",option_b:"To tune hyperparameters and prevent overfitting",option_c:"To provide the final performance estimate",option_d:"To augment the training data",correct_answer:"B",difficulty:0.3,concept:"Methodology",explanation:"The validation set is used during training to tune hyperparameters and monitor for overfitting.",hint:"It's the 'practice test' before the real test."},
  {id:32,question:"What is boosting in ensemble learning?",option_a:"Training models in parallel on random subsets",option_b:"Sequentially training models where each corrects the previous one's errors",option_c:"Using the same model multiple times",option_d:"Increasing the learning rate over time",correct_answer:"B",difficulty:0.5,concept:"Ensemble",explanation:"Boosting builds models sequentially, with each new model focusing on the mistakes of previous ones.",hint:"Each new model tries to fix what the previous ones got wrong."},
  {id:33,question:"What is the difference between bagging and boosting?",option_a:"Bagging is sequential; boosting is parallel",option_b:"Bagging trains models independently in parallel; boosting trains them sequentially",option_c:"They are the same technique",option_d:"Bagging uses neural networks; boosting uses decision trees",correct_answer:"B",difficulty:0.55,concept:"Ensemble",explanation:"Bagging (e.g., Random Forest) trains models independently on random subsets; boosting trains them sequentially to correct errors.",hint:"One is parallel and independent; the other is sequential and corrective."},
  {id:34,question:"What is a recurrent neural network (RNN) best suited for?",option_a:"Image classification",option_b:"Sequential data like time series or text",option_c:"Tabular data",option_d:"Static pattern recognition",correct_answer:"B",difficulty:0.4,concept:"Neural Networks",explanation:"RNNs have recurrent connections that maintain a hidden state, making them suitable for sequential data.",hint:"This network has a 'memory' of what it has seen before."},
  {id:35,question:"What is the purpose of data augmentation?",option_a:"To reduce the dataset size",option_b:"To artificially increase training data diversity",option_c:"To normalize the data",option_d:"To remove outliers",correct_answer:"B",difficulty:0.3,concept:"Methodology",explanation:"Data augmentation creates modified versions of training data (rotations, flips, etc.) to improve generalization.",hint:"Making more training examples from the ones you already have."},
  {id:36,question:"What is the difference between generative and discriminative models?",option_a:"Generative models are always better",option_b:"Generative models learn the data distribution; discriminative models learn the decision boundary",option_c:"Discriminative models can generate new data",option_d:"There is no meaningful difference",correct_answer:"B",difficulty:0.6,concept:"Theory",explanation:"Generative models learn P(X|Y) and P(Y); discriminative models learn P(Y|X) directly.",hint:"One learns how data is created; the other learns how to tell classes apart."},
  {id:37,question:"What is early stopping?",option_a:"Stopping training when the dataset runs out",option_b:"Stopping training when validation performance stops improving",option_c:"Using fewer epochs than planned",option_d:"Reducing the learning rate to zero",correct_answer:"B",difficulty:0.35,concept:"Regularization",explanation:"Early stopping monitors validation loss and stops training when it begins to increase, preventing overfitting.",hint:"Stop before you memorize the training data too well."},
  {id:38,question:"What is a confusion matrix?",option_a:"A matrix of model weights",option_b:"A table showing TP, TN, FP, and FN for classification results",option_c:"A correlation matrix of features",option_d:"A matrix used in PCA",correct_answer:"B",difficulty:0.3,concept:"Metrics",explanation:"A confusion matrix summarizes classification results by showing counts of correct and incorrect predictions for each class.",hint:"It shows where your classifier gets 'confused'."},
  {id:39,question:"What is the No Free Lunch theorem?",option_a:"All algorithms perform equally on all problems",option_b:"No single algorithm is best for every problem",option_c:"Free models always underperform paid ones",option_d:"Simple models always beat complex ones",correct_answer:"B",difficulty:0.65,concept:"Theory",explanation:"The No Free Lunch theorem states that no algorithm universally outperforms all others across all possible problems.",hint:"There's no magic algorithm that wins every time."},
  {id:40,question:"What is stochastic gradient descent (SGD)?",option_a:"Gradient descent using the entire dataset",option_b:"Gradient descent using a random single sample or mini-batch per update",option_c:"A gradient-free optimization method",option_d:"Gradient descent with a fixed learning rate",correct_answer:"B",difficulty:0.4,concept:"Optimization",explanation:"SGD updates parameters using gradients computed on random subsets of data, making training faster and adding noise that can help escape local minima.",hint:"Instead of looking at all data, you take a random sample each step."},
  {id:41,question:"What is the purpose of weight initialization in neural networks?",option_a:"To set all weights to zero",option_b:"To set starting weights that enable effective gradient flow",option_c:"To freeze the model parameters",option_d:"To determine the network architecture",correct_answer:"B",difficulty:0.5,concept:"Neural Networks",explanation:"Proper initialization (e.g., Xavier, He) ensures gradients neither vanish nor explode at the start of training.",hint:"How you start matters — bad initial weights can doom training."},
  {id:42,question:"What is the difference between parametric and non-parametric models?",option_a:"Parametric models have no parameters",option_b:"Parametric models have a fixed number of parameters; non-parametric models grow with data",option_c:"Non-parametric models are always better",option_d:"Parametric models require more data",correct_answer:"B",difficulty:0.5,concept:"Theory",explanation:"Parametric models (e.g., linear regression) have fixed parameters; non-parametric models (e.g., KNN) adapt complexity to data size.",hint:"One has a fixed form; the other is more flexible."},
  {id:43,question:"What is LSTM (Long Short-Term Memory)?",option_a:"A type of CNN",option_b:"An RNN variant with gates that control information flow for long sequences",option_c:"A loss function",option_d:"A data preprocessing technique",correct_answer:"B",difficulty:0.55,concept:"Neural Networks",explanation:"LSTM uses forget, input, and output gates to selectively remember or forget information over long sequences.",hint:"It solves the problem of forgetting in regular RNNs."},
  {id:44,question:"What is the purpose of the attention mechanism?",option_a:"To reduce model size",option_b:"To allow models to focus on relevant parts of the input",option_c:"To speed up training",option_d:"To normalize inputs",correct_answer:"B",difficulty:0.6,concept:"Neural Networks",explanation:"Attention mechanisms let models weigh different parts of the input differently, focusing on what's most relevant.",hint:"Like human attention — you focus on what matters most."},
  {id:45,question:"What is gradient clipping?",option_a:"Removing gradients below a threshold",option_b:"Limiting gradient values to prevent exploding gradients",option_c:"Clipping the input data",option_d:"A type of regularization",correct_answer:"B",difficulty:0.5,concept:"Optimization",explanation:"Gradient clipping caps gradient values at a maximum threshold to prevent training instability from exploding gradients.",hint:"Put a speed limit on how fast weights can change."},
  {id:46,question:"What is the difference between online and batch learning?",option_a:"Online learning uses all data at once; batch learning uses one sample",option_b:"Online learning updates with each new sample; batch learning uses the full dataset",option_c:"They are identical",option_d:"Batch learning is always online",correct_answer:"B",difficulty:0.4,concept:"Methodology",explanation:"Online learning updates the model incrementally with each new data point; batch learning uses the entire dataset per update.",hint:"One learns continuously; the other learns in bulk."},
  {id:47,question:"What is a hyperparameter?",option_a:"A parameter learned during training",option_b:"A parameter set before training that controls the learning process",option_c:"The output of the model",option_d:"A feature of the dataset",correct_answer:"B",difficulty:0.25,concept:"Methodology",explanation:"Hyperparameters (learning rate, number of layers, etc.) are set before training and control how the model learns.",hint:"You choose these before training starts — the model doesn't learn them."},
  {id:48,question:"What is the elbow method in clustering?",option_a:"A method to determine the optimal learning rate",option_b:"A method to find the optimal number of clusters by plotting inertia",option_c:"A technique for feature selection",option_d:"A regularization technique",correct_answer:"B",difficulty:0.4,concept:"Unsupervised Learning",explanation:"The elbow method plots the sum of squared distances vs K and looks for the 'elbow' point where adding more clusters gives diminishing returns.",hint:"Look for the bend in the curve — that's your sweet spot."},
  {id:49,question:"What is the difference between hard and soft clustering?",option_a:"Hard clustering is more accurate",option_b:"Hard clustering assigns each point to one cluster; soft clustering gives probability of membership",option_c:"Soft clustering always uses more clusters",option_d:"There is no difference",correct_answer:"B",difficulty:0.45,concept:"Unsupervised Learning",explanation:"Hard clustering (K-Means) assigns definite cluster labels; soft clustering (GMM) gives probability distributions over clusters.",hint:"One says 'you belong here'; the other says 'you probably belong here'."},
  {id:50,question:"What is the purpose of the kernel trick in SVM?",option_a:"To speed up training",option_b:"To implicitly map data to a higher-dimensional space for non-linear separation",option_c:"To reduce the number of features",option_d:"To initialize the support vectors",correct_answer:"B",difficulty:0.6,concept:"Supervised Learning",explanation:"The kernel trick computes dot products in a higher-dimensional space without explicitly transforming the data.",hint:"It lets you draw non-linear boundaries without the computational cost."},
  {id:51,question:"What is Adam optimizer?",option_a:"A type of neural network",option_b:"An adaptive learning rate optimization algorithm combining momentum and RMSProp",option_c:"A regularization technique",option_d:"A loss function",correct_answer:"B",difficulty:0.5,concept:"Optimization",explanation:"Adam (Adaptive Moment Estimation) adapts learning rates for each parameter using first and second moment estimates.",hint:"It's like gradient descent with a smart, adaptive step size."},
  {id:52,question:"What is the purpose of feature scaling?",option_a:"To add new features",option_b:"To normalize feature ranges so no single feature dominates the learning",option_c:"To remove irrelevant features",option_d:"To increase the dataset size",correct_answer:"B",difficulty:0.3,concept:"Methodology",explanation:"Feature scaling (standardization, normalization) ensures all features contribute equally to the model.",hint:"Make sure all features are on a level playing field."},
  {id:53,question:"What is XGBoost?",option_a:"A deep learning framework",option_b:"An optimized gradient boosting algorithm with regularization",option_c:"A clustering algorithm",option_d:"A dimensionality reduction method",correct_answer:"B",difficulty:0.45,concept:"Ensemble",explanation:"XGBoost is an efficient implementation of gradient boosting with built-in regularization and parallel processing.",hint:"It's the 'extreme' version of gradient boosting — fast and powerful."},
  {id:54,question:"What is the difference between classification and regression?",option_a:"Classification predicts continuous values; regression predicts categories",option_b:"Classification predicts categories; regression predicts continuous values",option_c:"They are the same thing",option_d:"Classification is unsupervised; regression is supervised",correct_answer:"B",difficulty:0.2,concept:"Supervised Learning",explanation:"Classification predicts discrete class labels; regression predicts continuous numerical values.",hint:"One answers 'what category?' and the other answers 'how much?'"},
  {id:55,question:"What is the purpose of a GAN (Generative Adversarial Network)?",option_a:"To classify images",option_b:"To generate realistic synthetic data through adversarial training",option_c:"To cluster data points",option_d:"To reduce dimensionality",correct_answer:"B",difficulty:0.55,concept:"Neural Networks",explanation:"GANs consist of a generator and discriminator that compete, with the generator learning to create realistic data.",hint:"Two networks play a game — one creates fakes, the other tries to catch them."},
  {id:56,question:"What is the information gain in decision trees?",option_a:"The total amount of data",option_b:"The reduction in entropy after splitting on a feature",option_c:"The accuracy improvement",option_d:"The number of features used",correct_answer:"B",difficulty:0.45,concept:"Supervised Learning",explanation:"Information gain measures how much a feature reduces uncertainty (entropy) when used to split the data.",hint:"Which feature gives you the most useful information for making a decision?"},
  {id:57,question:"What is the Bayesian approach to machine learning?",option_a:"Using only frequentist statistics",option_b:"Incorporating prior knowledge and updating beliefs with data using Bayes' theorem",option_c:"Ignoring prior information",option_d:"Using only neural networks",correct_answer:"B",difficulty:0.6,concept:"Theory",explanation:"Bayesian ML uses prior distributions over parameters and updates them with observed data to get posterior distributions.",hint:"Start with what you believe, then update as you see evidence."},
  {id:58,question:"What is multi-task learning?",option_a:"Training one model per task",option_b:"Training a single model to perform multiple related tasks simultaneously",option_c:"Using multiple datasets for one task",option_d:"Sequential training on different tasks",correct_answer:"B",difficulty:0.5,concept:"Methodology",explanation:"Multi-task learning trains a shared model on multiple tasks, allowing knowledge transfer between them.",hint:"One model, many jobs — and they help each other."},
  {id:59,question:"What is the purpose of residual connections (skip connections)?",option_a:"To skip training certain layers",option_b:"To allow gradients to flow directly through the network, enabling deeper architectures",option_c:"To reduce the number of parameters",option_d:"To speed up inference",correct_answer:"B",difficulty:0.55,concept:"Neural Networks",explanation:"Residual connections add the input of a layer to its output, helping gradients flow and enabling very deep networks.",hint:"A shortcut that lets information bypass layers."},
  {id:60,question:"What is the difference between momentum and learning rate in optimization?",option_a:"They are the same thing",option_b:"Learning rate controls step size; momentum accumulates past gradients for smoother updates",option_c:"Momentum replaces the learning rate",option_d:"Learning rate is for CNNs; momentum is for RNNs",correct_answer:"B",difficulty:0.5,concept:"Optimization",explanation:"Learning rate sets the step size; momentum adds a fraction of the previous update to smooth oscillations and accelerate convergence.",hint:"One controls speed; the other adds inertia to keep moving in the right direction."},
  {id:61,question:"Which of the following is a sign of overfitting?",option_a:"High training accuracy and high test accuracy",option_b:"High training accuracy but low test accuracy",option_c:"Low training accuracy and low test accuracy",option_d:"Low training accuracy but high test accuracy",correct_answer:"B",difficulty:0.25,concept:"Overfitting",explanation:"Overfitting is characterized by a model performing well on training data but poorly on unseen test data, indicating it memorized rather than learned.",hint:"The model aces practice tests but fails the real exam."},
  {id:62,question:"How does increasing training data help with overfitting?",option_a:"It makes the model more complex",option_b:"It provides more patterns for the model to memorize",option_c:"It helps the model learn more general patterns instead of noise",option_d:"It has no effect on overfitting",correct_answer:"C",difficulty:0.35,concept:"Overfitting",explanation:"More training data makes it harder for the model to memorize specific examples, forcing it to learn generalizable patterns.",hint:"With more examples, the model can't just memorize — it has to understand."},
  {id:63,question:"What is early stopping in the context of overfitting?",option_a:"Stopping the training before the model converges",option_b:"Halting training when validation performance stops improving",option_c:"Removing features before training",option_d:"Reducing the learning rate to zero",correct_answer:"B",difficulty:0.4,concept:"Overfitting",explanation:"Early stopping monitors validation loss and stops training when it begins to increase, preventing the model from overfitting to training data.",hint:"Stop before the model starts memorizing — watch the validation score."},
  {id:64,question:"Which model is more likely to overfit: a decision tree with max depth 3 or max depth 100?",option_a:"Max depth 3",option_b:"Max depth 100",option_c:"Both equally",option_d:"Neither can overfit",correct_answer:"B",difficulty:0.3,concept:"Overfitting",explanation:"A deeper decision tree can create more complex boundaries, making it more prone to fitting noise in the training data.",hint:"More complexity means more room to memorize specific data points."},
  {id:65,question:"What is the relationship between model complexity and overfitting?",option_a:"Higher complexity always prevents overfitting",option_b:"Higher complexity increases the risk of overfitting",option_c:"Complexity has no effect on overfitting",option_d:"Lower complexity always causes overfitting",correct_answer:"B",difficulty:0.3,concept:"Overfitting",explanation:"More complex models have more parameters and can fit training data more closely, including noise, which increases overfitting risk.",hint:"Think about the bias-variance tradeoff — more complexity means more variance."},
  // === Ensemble (need 5 more) ===
  {id:66,question:"What is the main idea behind bagging (Bootstrap Aggregating)?",option_a:"Training models sequentially to correct previous errors",option_b:"Training multiple models on random subsets of data and averaging their predictions",option_c:"Using a single model with multiple loss functions",option_d:"Selecting the best single model from a pool",correct_answer:"B",difficulty:0.35,concept:"Ensemble",explanation:"Bagging trains multiple models independently on bootstrapped samples and aggregates their predictions to reduce variance.",hint:"Think about creating many random samples and combining their wisdom."},
  {id:67,question:"How does boosting differ from bagging?",option_a:"Boosting trains models in parallel; bagging trains them sequentially",option_b:"Boosting trains models sequentially, each correcting the errors of the previous one",option_c:"Boosting uses only decision trees; bagging can use any model",option_d:"There is no difference between boosting and bagging",correct_answer:"B",difficulty:0.45,concept:"Ensemble",explanation:"Boosting builds models sequentially, where each new model focuses on the mistakes made by the previous models, reducing bias.",hint:"Each new model tries to fix what the last one got wrong."},
  {id:68,question:"What is the key advantage of XGBoost over traditional gradient boosting?",option_a:"It uses neural networks internally",option_b:"It includes regularization terms and efficient tree pruning",option_c:"It only works with linear models",option_d:"It doesn't require hyperparameter tuning",correct_answer:"B",difficulty:0.55,concept:"Ensemble",explanation:"XGBoost adds L1/L2 regularization to the objective function and uses efficient tree pruning, making it faster and less prone to overfitting.",hint:"It's gradient boosting with extra tricks to prevent overfitting and speed things up."},
  {id:69,question:"In a Random Forest, what makes each tree different from the others?",option_a:"Each tree uses a different algorithm",option_b:"Each tree is trained on a random subset of data and features",option_c:"Each tree has a different number of layers",option_d:"Each tree uses a different loss function",correct_answer:"B",difficulty:0.3,concept:"Ensemble",explanation:"Random Forest introduces randomness by bootstrapping data samples and randomly selecting feature subsets at each split.",hint:"Two sources of randomness: random data samples and random features."},
  {id:70,question:"What is stacking in ensemble learning?",option_a:"Stacking multiple datasets on top of each other",option_b:"Using the predictions of base models as input features for a meta-model",option_c:"Training the same model multiple times with different seeds",option_d:"Applying the same model to different subsets of features",correct_answer:"B",difficulty:0.5,concept:"Ensemble",explanation:"Stacking trains a meta-learner that takes the outputs of several base models as inputs to make the final prediction.",hint:"The base models make predictions, then another model learns from those predictions."},
  // === Methodology (need 1 more) ===
  {id:71,question:"What is the purpose of stratified sampling in machine learning?",option_a:"To increase the size of the dataset",option_b:"To ensure each class is proportionally represented in train/test splits",option_c:"To remove outliers from the data",option_d:"To normalize feature values",correct_answer:"B",difficulty:0.35,concept:"Methodology",explanation:"Stratified sampling maintains the original class distribution in each split, which is especially important for imbalanced datasets.",hint:"If 10% of your data is class A, each fold should also have about 10% class A."},
  // === Metrics (need 4 more) ===
  {id:72,question:"What does the ROC curve plot?",option_a:"Precision vs Recall",option_b:"True Positive Rate vs False Positive Rate",option_c:"Accuracy vs Loss",option_d:"Bias vs Variance",correct_answer:"B",difficulty:0.4,concept:"Metrics",explanation:"The ROC curve plots TPR (sensitivity) against FPR (1-specificity) at various classification thresholds.",hint:"It shows the tradeoff between catching positives and falsely flagging negatives."},
  {id:73,question:"What does AUC (Area Under the ROC Curve) represent?",option_a:"The total number of correct predictions",option_b:"The probability that the model ranks a random positive higher than a random negative",option_c:"The average precision across all thresholds",option_d:"The maximum achievable accuracy",correct_answer:"B",difficulty:0.5,concept:"Metrics",explanation:"AUC measures the model's ability to distinguish between classes. An AUC of 1.0 means perfect discrimination.",hint:"If you pick one positive and one negative example at random, how often does the model rank the positive higher?"},
  {id:74,question:"When should you use RMSE instead of MAE?",option_a:"When all errors should be treated equally",option_b:"When large errors should be penalized more heavily",option_c:"When the data has many outliers",option_d:"When the target variable is categorical",correct_answer:"B",difficulty:0.4,concept:"Metrics",explanation:"RMSE squares errors before averaging, so large errors contribute disproportionately. This is useful when big mistakes are especially costly.",hint:"Squaring makes big errors stand out even more."},
  {id:75,question:"What is the Matthews Correlation Coefficient (MCC) used for?",option_a:"Measuring regression performance",option_b:"Evaluating binary classification that accounts for all four confusion matrix values",option_c:"Calculating the learning rate",option_d:"Measuring feature importance",correct_answer:"B",difficulty:0.55,concept:"Metrics",explanation:"MCC uses TP, TN, FP, and FN to produce a balanced measure that works well even with imbalanced classes. It ranges from -1 to +1.",hint:"Unlike accuracy, it considers all four quadrants of the confusion matrix."},
  // === Optimization (need 3 more) ===
  {id:76,question:"What is the vanishing gradient problem?",option_a:"Gradients become too large during training",option_b:"Gradients become extremely small, preventing deep layers from learning",option_c:"The learning rate vanishes over time",option_d:"The model's weights become zero",correct_answer:"B",difficulty:0.5,concept:"Optimization",explanation:"In deep networks with sigmoid/tanh activations, gradients can shrink exponentially as they propagate backward, making early layers learn very slowly.",hint:"Each layer multiplies a small number, and many small numbers multiplied together become tiny."},
  {id:77,question:"What is the Adam optimizer?",option_a:"A variant of gradient descent that uses only momentum",option_b:"An optimizer that combines momentum and adaptive learning rates",option_c:"A second-order optimization method",option_d:"An optimizer designed only for convolutional networks",correct_answer:"B",difficulty:0.45,concept:"Optimization",explanation:"Adam (Adaptive Moment Estimation) maintains running averages of both gradients and squared gradients, adapting the learning rate for each parameter.",hint:"It's like momentum + RMSProp combined into one optimizer."},
  {id:78,question:"What is a learning rate scheduler?",option_a:"A tool that selects the best optimizer",option_b:"A strategy that adjusts the learning rate during training",option_c:"A method to initialize weights",option_d:"A technique to augment training data",correct_answer:"B",difficulty:0.4,concept:"Optimization",explanation:"Learning rate schedulers reduce the learning rate over time (e.g., step decay, cosine annealing) to help the model converge more precisely.",hint:"Start with big steps, then take smaller steps as you get closer to the minimum."},
  // === Regularization (need 6 more) ===
  {id:79,question:"What is data augmentation and how does it relate to regularization?",option_a:"It's a method to reduce the dataset size",option_b:"It artificially increases training data variety, acting as an implicit regularizer",option_c:"It replaces the need for a validation set",option_d:"It only works with text data",correct_answer:"B",difficulty:0.35,concept:"Regularization",explanation:"Data augmentation creates modified copies of training data (rotations, flips, etc.), increasing diversity and reducing overfitting.",hint:"More varied training examples make it harder for the model to memorize."},
  {id:80,question:"What is batch normalization?",option_a:"Normalizing the entire dataset before training",option_b:"Normalizing layer inputs within each mini-batch during training",option_c:"Reducing the batch size to prevent overfitting",option_d:"Sorting data into batches by class",correct_answer:"B",difficulty:0.45,concept:"Regularization",explanation:"Batch normalization normalizes activations within each mini-batch, stabilizing training and acting as a mild regularizer.",hint:"It keeps the internal values of the network from shifting too much during training."},
  {id:81,question:"What is weight decay?",option_a:"Gradually removing neurons during training",option_b:"Adding a penalty proportional to weight magnitude to the loss function",option_c:"Decreasing the learning rate over time",option_d:"Reducing the number of training epochs",correct_answer:"B",difficulty:0.4,concept:"Regularization",explanation:"Weight decay is equivalent to L2 regularization — it penalizes large weights to keep the model simpler and reduce overfitting.",hint:"It's another name for L2 regularization applied during optimization."},
  {id:82,question:"How does elastic net regularization work?",option_a:"It uses only L1 regularization",option_b:"It combines both L1 and L2 regularization penalties",option_c:"It removes features with low variance",option_d:"It applies regularization only to the output layer",correct_answer:"B",difficulty:0.5,concept:"Regularization",explanation:"Elastic net adds both L1 (sparsity) and L2 (small weights) penalties, combining the benefits of Lasso and Ridge regression.",hint:"Why choose one when you can have both L1 and L2?"},
  {id:83,question:"What is the effect of increasing the regularization strength (lambda)?",option_a:"The model becomes more complex",option_b:"The model becomes simpler with smaller weights",option_c:"Training speed increases",option_d:"The model ignores the training data",correct_answer:"B",difficulty:0.35,concept:"Regularization",explanation:"Higher lambda penalizes large weights more aggressively, pushing the model toward simpler solutions with smaller parameters.",hint:"More penalty = more pressure to keep things simple."},
  {id:84,question:"What is label smoothing?",option_a:"Replacing hard labels (0/1) with soft labels (e.g., 0.1/0.9)",option_b:"Smoothing the loss function curve",option_c:"Averaging predictions across multiple epochs",option_d:"Removing noisy labels from the dataset",correct_answer:"A",difficulty:0.5,concept:"Regularization",explanation:"Label smoothing replaces hard targets with soft targets, preventing the model from becoming overconfident and acting as a regularizer.",hint:"Instead of saying 'definitely class A', say 'probably class A'."},
  // === Supervised Learning (need 4 more) ===
  {id:85,question:"What is the difference between classification and regression?",option_a:"Classification predicts categories; regression predicts continuous values",option_b:"Classification uses unsupervised learning; regression uses supervised learning",option_c:"Classification works only with images; regression works only with numbers",option_d:"There is no difference",correct_answer:"A",difficulty:0.2,concept:"Supervised Learning",explanation:"Classification assigns inputs to discrete categories (e.g., spam/not spam), while regression predicts continuous values (e.g., house price).",hint:"One gives you a label; the other gives you a number."},
  {id:86,question:"What is a support vector machine (SVM)?",option_a:"A clustering algorithm",option_b:"A classifier that finds the optimal hyperplane separating classes with maximum margin",option_c:"A dimensionality reduction technique",option_d:"A type of neural network",correct_answer:"B",difficulty:0.4,concept:"Supervised Learning",explanation:"SVM finds the hyperplane that maximizes the margin between classes, using support vectors (closest points) to define the boundary.",hint:"It draws the widest possible street between two groups of points."},
  {id:87,question:"What is the kernel trick in SVM?",option_a:"A method to speed up training",option_b:"Mapping data to a higher-dimensional space to make it linearly separable",option_c:"Reducing the number of support vectors",option_d:"A technique for feature selection",correct_answer:"B",difficulty:0.55,concept:"Supervised Learning",explanation:"The kernel trick implicitly maps data to a higher-dimensional space where a linear separator can be found, without explicitly computing the transformation.",hint:"Can't draw a straight line? Go to a higher dimension where you can."},
  {id:88,question:"What is Naive Bayes and why is it called 'naive'?",option_a:"It's naive because it assumes all features are independent given the class",option_b:"It's naive because it doesn't use training data",option_c:"It's naive because it only works with binary features",option_d:"It's naive because it ignores the target variable",correct_answer:"A",difficulty:0.35,concept:"Supervised Learning",explanation:"Naive Bayes applies Bayes' theorem with the 'naive' assumption that features are conditionally independent given the class label.",hint:"The 'naive' part is about a simplifying assumption regarding feature relationships."},
  // === Theory (need 4 more) ===
  {id:89,question:"What is the No Free Lunch theorem in machine learning?",option_a:"All algorithms perform equally well on all problems",option_b:"No single algorithm is universally best for all problems",option_c:"Free models always underperform paid ones",option_d:"Simple models always outperform complex ones",correct_answer:"B",difficulty:0.5,concept:"Theory",explanation:"The No Free Lunch theorem states that no algorithm is optimal for every problem — the best choice depends on the specific data and task.",hint:"There's no magic algorithm that wins everywhere."},
  {id:90,question:"What is the curse of dimensionality?",option_a:"Having too few features",option_b:"As dimensions increase, data becomes sparse and distances lose meaning",option_c:"Models become faster with more features",option_d:"High-dimensional data is always better",correct_answer:"B",difficulty:0.45,concept:"Theory",explanation:"In high-dimensional spaces, data points become increasingly sparse, making it harder to find meaningful patterns and requiring exponentially more data.",hint:"More dimensions = more empty space = harder to find neighbors."},
  {id:91,question:"What is Occam's Razor in the context of machine learning?",option_a:"Always use the most complex model available",option_b:"Prefer the simplest model that adequately explains the data",option_c:"Use as many features as possible",option_d:"Train for as many epochs as possible",correct_answer:"B",difficulty:0.3,concept:"Theory",explanation:"Occam's Razor suggests choosing the simplest hypothesis that fits the data, as simpler models are less likely to overfit and more likely to generalize.",hint:"When in doubt, keep it simple."},
  {id:92,question:"What is the difference between parametric and non-parametric models?",option_a:"Parametric models have no parameters",option_b:"Parametric models have a fixed number of parameters; non-parametric models grow with data",option_c:"Non-parametric models are always better",option_d:"Parametric models don't need training data",correct_answer:"B",difficulty:0.4,concept:"Theory",explanation:"Parametric models (e.g., linear regression) have a fixed structure, while non-parametric models (e.g., KNN) can grow in complexity with more data.",hint:"One has a fixed form; the other adapts its complexity to the data."},
  // === Unsupervised Learning (need 6 more) ===
  {id:93,question:"What is hierarchical clustering?",option_a:"Clustering that requires specifying K in advance",option_b:"Clustering that builds a tree-like hierarchy of nested clusters",option_c:"Clustering that only works with labeled data",option_d:"Clustering that uses neural networks",correct_answer:"B",difficulty:0.4,concept:"Unsupervised Learning",explanation:"Hierarchical clustering creates a dendrogram showing nested groupings, either by merging (agglomerative) or splitting (divisive) clusters.",hint:"Think of a family tree of clusters — groups within groups."},
  {id:94,question:"What is DBSCAN and how does it differ from K-Means?",option_a:"DBSCAN requires specifying the number of clusters; K-Means doesn't",option_b:"DBSCAN finds clusters of arbitrary shape based on density; K-Means finds spherical clusters",option_c:"DBSCAN is a supervised algorithm",option_d:"There is no difference",correct_answer:"B",difficulty:0.45,concept:"Unsupervised Learning",explanation:"DBSCAN groups points that are closely packed together and can find arbitrarily shaped clusters, unlike K-Means which assumes spherical clusters.",hint:"One looks for dense regions; the other looks for round groups."},
  {id:95,question:"What is t-SNE used for?",option_a:"Training neural networks",option_b:"Visualizing high-dimensional data in 2D or 3D",option_c:"Feature selection",option_d:"Predicting continuous values",correct_answer:"B",difficulty:0.4,concept:"Unsupervised Learning",explanation:"t-SNE (t-distributed Stochastic Neighbor Embedding) is a dimensionality reduction technique specifically designed for visualization of high-dimensional data.",hint:"It helps you 'see' patterns in data that has too many dimensions to plot directly."},
  {id:96,question:"What is an autoencoder?",option_a:"A supervised classification model",option_b:"A neural network that learns to compress and reconstruct data",option_c:"An optimization algorithm",option_d:"A type of decision tree",correct_answer:"B",difficulty:0.45,concept:"Unsupervised Learning",explanation:"Autoencoders learn a compressed representation (encoding) of data and then reconstruct it, useful for dimensionality reduction and feature learning.",hint:"It squeezes data through a bottleneck and tries to rebuild it."},
  {id:97,question:"What is the elbow method in clustering?",option_a:"A method to select the best features",option_b:"A technique to determine the optimal number of clusters by plotting inertia vs K",option_c:"A way to initialize cluster centroids",option_d:"A method to evaluate classification accuracy",correct_answer:"B",difficulty:0.35,concept:"Unsupervised Learning",explanation:"The elbow method plots the within-cluster sum of squares against K and looks for the 'elbow' point where adding more clusters gives diminishing returns.",hint:"Plot a curve and look for where it bends — that's your sweet spot for K."},
  {id:98,question:"What is anomaly detection and which unsupervised methods are used for it?",option_a:"Detecting errors in code using supervised learning",option_b:"Identifying unusual data points using methods like Isolation Forest or autoencoders",option_c:"Finding the most common patterns in data",option_d:"Labeling data for classification",correct_answer:"B",difficulty:0.45,concept:"Unsupervised Learning",explanation:"Anomaly detection identifies data points that deviate significantly from normal patterns, using methods like Isolation Forest, One-Class SVM, or autoencoders.",hint:"Finding the needle in the haystack — what doesn't belong?"},
  // === Additional Methodology ===
  {id:99,question:"What is feature engineering?",option_a:"Automatically training a model",option_b:"The process of creating new features from raw data to improve model performance",option_c:"Removing all features from the dataset",option_d:"A type of neural network architecture",correct_answer:"B",difficulty:0.3,concept:"Methodology",explanation:"Feature engineering involves creating, transforming, or selecting features that help the model learn better patterns from the data.",hint:"It's the art of turning raw data into useful inputs for your model."},
  {id:100,question:"What is the difference between grid search and random search for hyperparameter tuning?",option_a:"Grid search is always faster",option_b:"Grid search tries all combinations; random search samples randomly from the parameter space",option_c:"Random search is deterministic; grid search is stochastic",option_d:"They produce identical results",correct_answer:"B",difficulty:0.4,concept:"Methodology",explanation:"Grid search exhaustively evaluates all parameter combinations, while random search samples randomly, often finding good solutions faster in high-dimensional spaces.",hint:"One is systematic but slow; the other is random but efficient."},
  // === Additional Neural Networks ===
  {id:101,question:"What is transfer learning?",option_a:"Moving data between databases",option_b:"Using a pre-trained model on a new but related task",option_c:"Training a model from scratch on every new task",option_d:"Transferring weights between different optimizers",correct_answer:"B",difficulty:0.35,concept:"Neural Networks",explanation:"Transfer learning leverages knowledge from a model trained on a large dataset (e.g., ImageNet) and fine-tunes it for a specific task with less data.",hint:"Why start from zero when someone already learned the basics?"},
  {id:102,question:"What is the difference between a CNN and an RNN?",option_a:"CNNs are for sequential data; RNNs are for images",option_b:"CNNs use convolutional layers for spatial patterns; RNNs use recurrent connections for sequential data",option_c:"CNNs are unsupervised; RNNs are supervised",option_d:"There is no difference",correct_answer:"B",difficulty:0.4,concept:"Neural Networks",explanation:"CNNs excel at spatial data (images) using convolutional filters, while RNNs process sequential data (text, time series) using recurrent connections that maintain memory.",hint:"One sees patterns in space; the other remembers patterns over time."},
  // === Additional Overfitting (need 4 more to reach 10) ===
  {id:103,question:"What is the difference between overfitting and underfitting?",option_a:"Overfitting means the model is too simple; underfitting means it's too complex",option_b:"Overfitting captures noise in training data; underfitting fails to capture the underlying pattern",option_c:"They are the same thing",option_d:"Overfitting only happens with neural networks",correct_answer:"B",difficulty:0.25,concept:"Overfitting",explanation:"Overfitting occurs when a model learns noise along with patterns (high variance), while underfitting occurs when a model is too simple to capture patterns (high bias).",hint:"One tries too hard; the other doesn't try hard enough."},
  {id:104,question:"How can you detect overfitting during training?",option_a:"Training loss keeps decreasing while validation loss starts increasing",option_b:"Both training and validation loss decrease together",option_c:"Training loss increases while validation loss decreases",option_d:"Neither loss changes during training",correct_answer:"A",difficulty:0.35,concept:"Overfitting",explanation:"The classic sign of overfitting is a growing gap between training loss (decreasing) and validation loss (increasing), indicating the model is memorizing rather than learning.",hint:"Watch the gap between two loss curves — when they diverge, trouble begins."},
  {id:105,question:"Why does a small training dataset increase the risk of overfitting?",option_a:"Small datasets always have more noise",option_b:"With fewer examples, the model can memorize specific patterns instead of learning general ones",option_c:"Small datasets make training faster, causing overfitting",option_d:"Dataset size has no effect on overfitting",correct_answer:"B",difficulty:0.3,concept:"Overfitting",explanation:"With limited data, models can easily memorize individual examples and their noise, rather than extracting generalizable patterns that work on new data.",hint:"It's easier to memorize 10 answers than 10,000."},
  {id:106,question:"What role does the validation set play in preventing overfitting?",option_a:"It provides additional training data",option_b:"It serves as an independent check on model generalization during training",option_c:"It replaces the need for a test set",option_d:"It is only used after training is complete",correct_answer:"B",difficulty:0.3,concept:"Overfitting",explanation:"The validation set provides an unbiased estimate of model performance during training, allowing you to detect overfitting and tune hyperparameters before final evaluation.",hint:"It's like a practice exam that tells you if you're actually learning or just memorizing."},
];



export function getQuestionsByConceptMap(): Record<string, Question[]> {
  const map: Record<string, Question[]> = {};
  for (const q of questions) {
    if (!map[q.concept]) map[q.concept] = [];
    map[q.concept].push(q);
  }
  return map;
}

export function getQuestionById(id: number): Question | undefined {
  return questions.find(q => q.id === id);
}
