id,question,option_a,option_b,option_c,option_d,correct_answer,difficulty,concept,explanation,hint
1,What is the main risk of a model that performs well on training data but poorly on test data?,Underfitting,Overfitting,Regularization,Normalization,B,0.2,Overfitting,Overfitting happens when a model learns the noise in the training data rather than the actual pattern.,Think about the difference between how a model performs on data it has seen (training) versus new data (testing).
2,Which metric is most appropriate for a binary classification problem with imbalanced classes?,Accuracy,F1-Score,Mean Squared Error,R-Squared,B,0.4,Metrics,Accuracy can be misleading in imbalanced datasets; F1-Score balances Precision and Recall.,"When classes are imbalanced, accuracy can be misleading. Consider metrics that account for both precision and recall."
3,"In Supervised Learning, the dataset consists of?",Input data only,Output labels only,Input data and Output labels,None of the above,C,0.1,Supervised,Supervised learning requires input-output pairs to map functions from inputs to labels.,Supervised learning uses labeled examples to learn patterns. What does 'supervised' imply about the data?
4,What does a high learning rate often lead to?,Faster convergence,Stable training,Overshooting the minimum,Vanishing gradients,C,0.3,Optimization,A learning rate that is too high can cause the optimizer to overshoot the global minimum and fail to converge.,Learning rate controls step size. Too high means overshooting; too low means slow convergence.
5,Which algorithm is typically used for clustering?,Linear Regression,Decision Trees,K-Means,Logistic Regression,C,0.2,Unsupervised,K-Means is a centroid-based algorithm used to partition data into K clusters.,Supervised learning uses labeled examples to learn patterns. What does 'supervised' imply about the data?
6,Which of the following is an example of Unsupervised Learning?,Spam detection,House price prediction,Customer segmentation,Medical diagnosis,C,0.2,Unsupervised,Customer segmentation often uses clustering to find hidden patterns without labeled data.,Supervised learning uses labeled examples to learn patterns. What does 'supervised' imply about the data?
7,What is the purpose of a validation set?,To train the model,To tune hyperparameters,To evaluate final performance,To preprocess data,B,0.3,Methodology,The validation set is used to tune hyperparameters and prevent overfitting before final testing.,Consider the key concept: Methodology. Think about how it relates to the question and options.
8,Which activation function outputs a value between 0 and 1?,ReLU,Tanh,Sigmoid,Leaky ReLU,C,0.3,Neural Networks,"The Sigmoid function squashes numbers to range [0, 1], making it useful for probability estimation.",Neural networks use activation functions to introduce non-linearity. Consider what Neural Networks does.
9,What happens to bias and variance when you increase model complexity?,Bias increases Variance decreases,Bias decreases Variance increases,Both increase,Both decrease,B,0.6,Overfitting,More complex models fit training data better (lower bias) but fluctuate more on unseen data (higher variance).,Think about the difference between how a model performs on data it has seen (training) versus new data (testing).
10,L1 Regularization is also known as?,Ridge Regression,Lasso Regression,Elastic Net,Dropout,B,0.5,Overfitting,Lasso (L1) adds the absolute value of magnitude of coefficient as penalty term to the loss function.,Think about the difference between how a model performs on data it has seen (training) versus new data (testing).
11,Which metric is used to evaluate a Regression model like $y = mx + c$?,Accuracy,F1-Score,Mean Squared Error (MSE),AUC-ROC,C,0.2,Metrics,MSE measures the average squared difference between the estimated values and the actual value.,"For regression problems, think about how to measure the difference between predicted and actual continuous values."
12,What is the role of the cost function $J(\theta)$?,To visualize data,To measure how wrong the model is,To initialize weights,To split the dataset,B,0.2,Optimization,The cost function quantifies the error between predicted and actual values.,The cost function measures prediction error. Lower cost means better predictions.
13,Which of the following is a parametric model?,K-Nearest Neighbors,Decision Trees,Linear Regression,Random Forest,C,0.5,Theory,Parametric models summarize data with a fixed set of parameters (like weights in Linear Regression).,Parametric models use a fixed set of parameters (like weights) to represent the data.
14,"In a Confusion Matrix, what does True Positive (TP) mean?",Correctly predicted positive,Incorrectly predicted positive,Correctly predicted negative,Incorrectly predicted negative,A,0.3,Metrics,True Positive means the model correctly predicted the positive class.,True Positive means the model correctly predicted the positive class. Think about what 'True' and 'Positive' mean.
15,Gradient Descent is an algorithm used for?,Clustering,Optimization,Regularization,Data Cleaning,B,0.2,Optimization,Gradient Descent is an iterative optimization algorithm for finding the local minimum of a differentiable function.,Gradient descent finds the minimum by following the steepest descent direction. SGD uses one sample at a time.
16,Which technique helps to reduce overfitting?,Adding more features,Increasing model complexity,Early Stopping,Removing the test set,C,0.4,Overfitting,Early stopping halts training when validation performance stops improving.,Think about the difference between how a model performs on data it has seen (training) versus new data (testing).
17,What does PCA (Principal Component Analysis) do?,Increases dimensionality,Reduces dimensionality,Classifies data,Clusters data,B,0.5,Unsupervised,PCA reduces the dimensionality of data while retaining most of the variation in the dataset.,Supervised learning uses labeled examples to learn patterns. What does 'supervised' imply about the data?
18,The vanishing gradient problem is most common in which activation function?,ReLU,Sigmoid,Leaky ReLU,Softmax,B,0.6,Neural Networks,"Sigmoid derivatives are small (<0.25), causing gradients to vanish as they propagate back through many layers.","Some activation functions have very small derivatives, causing gradients to shrink as they propagate backward."
19,What is the output of a Softmax function?,A single class label,A probability distribution,A regression value,A binary value,B,0.4,Neural Networks,Softmax converts a vector of numbers into a vector of probabilities summing to one.,Softmax converts raw scores into a probability distribution that sums to 1.
20,K-Fold Cross-Validation is used to?,Train the model faster,Estimate model performance reliably,Remove outliers,Increase dataset size,B,0.4,Methodology,K-Fold provides a more robust estimate of model performance by averaging results over K subsets.,Cross-validation splits data into K folds to get a more reliable performance estimate by averaging results.
21,Which of these is an ensemble learning method?,SVM,Decision Tree,Random Forest,Logistic Regression,C,0.4,Supervised,"Random Forest is an ensemble of Decision Trees, generally reducing variance.",Supervised learning uses labeled examples to learn patterns. What does 'supervised' imply about the data?
22,"In K-Means clustering, what does 'K' represent?",Number of iterations,Number of clusters,Number of data points,Learning rate,B,0.2,Unsupervised,K represents the number of clusters the algorithm will attempt to find.,Supervised learning uses labeled examples to learn patterns. What does 'supervised' imply about the data?
23,Which algorithm constructs a hyperplane to separate classes?,K-Means,SVM (Support Vector Machine),Apriori,Linear Regression,B,0.5,Supervised,SVM finds the hyperplane that maximizes the margin between the two classes.,Supervised learning uses labeled examples to learn patterns. What does 'supervised' imply about the data?
24,What is the range of the Tanh activation function?,0 to 1,-1 to 1,0 to infinity,-infinity to infinity,B,0.3,Neural Networks,"Tanh outputs values between -1 and 1, making it zero-centered unlike Sigmoid.","Tanh is similar to sigmoid but outputs values between -1 and 1, making it zero-centered."
25,Underfitting typically occurs when?,The model is too complex,The model is too simple,The dataset is too large,The learning rate is too low,B,0.2,Overfitting,Underfitting occurs when the model is not complex enough to capture the underlying pattern of the data.,Think about the difference between how a model performs on data it has seen (training) versus new data (testing).
26,Which value of AUC (Area Under Curve) indicates a random classifier?,0,0.5,1,0.1,B,0.5,Metrics,An AUC of 0.5 represents a model with no discrimination capacity (random guessing).,AUC measures the classifier's ability to distinguish between classes. What value would indicate random guessing?
27,Stochastic Gradient Descent (SGD) updates weights using?,The entire dataset,A single sample at a time,A mini-batch,The validation set,B,0.4,Optimization,"SGD updates parameters for each training example, making it faster but noisier.",Gradient descent finds the minimum by following the steepest descent direction. SGD uses one sample at a time.
28,Which problem does the 'Kernel Trick' solve in SVM?,Overfitting,Non-linear separability,High computational cost,Missing values,B,0.7,Supervised,The Kernel Trick allows SVMs to solve non-linear problems by mapping data to a higher-dimensional space.,Supervised learning uses labeled examples to learn patterns. What does 'supervised' imply about the data?
29,Bagging (Bootstrap Aggregating) is primarily used to reduce?,Bias,Variance,Computation time,Data noise,B,0.6,Theory,"Bagging reduces variance and helps to avoid overfitting (e.g., in Random Forests).",Ensemble methods combine multiple models. Random Forest uses many decision trees and averages their predictions.
30,Which layer in a CNN is used to reduce spatial dimensions?,Convolutional Layer,Pooling Layer,Fully Connected Layer,Dropout Layer,B,0.4,Neural Networks,Pooling layers (like Max Pooling) downsample the feature maps to reduce dimensions and computation.,Pooling layers reduce spatial dimensions by downsampling feature maps.
31,A decision tree with unlimited depth is prone to?,Underfitting,Overfitting,High Bias,Convergence,B,0.3,Supervised,"Unlimited depth allows the tree to memorize the training data, leading to overfitting.",Supervised learning uses labeled examples to learn patterns. What does 'supervised' imply about the data?
32,What is the main advantage of ReLU over Sigmoid?,Outputs are bounded,Fixes exploding gradients,Computationally efficient and avoids vanishing gradient,Smooth gradient,C,0.5,Neural Networks,"ReLU is faster to compute and does not saturate for positive values, avoiding the vanishing gradient problem.","Sigmoid outputs values between 0 and 1, making it useful for probability estimation."
33,"In NLP, what does 'Tokenization' refer to?",Splitting text into words or subwords,Removing stop words,Converting words to vectors,Translating text,A,0.3,NLP,Tokenization is the process of breaking text down into smaller units called tokens.,Tokenization breaks text into smaller units (tokens) like words or subwords for processing.
34,Which loss function is used for Binary Classification?,Mean Squared Error,Cross-Entropy Loss,Hinge Loss,Huber Loss,B,0.4,Optimization,Binary Cross-Entropy Loss is the standard loss function for binary classification problems.,Optimization aims to find the best parameters. Consider what Optimization means in the context of machine learning.
35,Which concept describes the trade-off between Precision and Recall?,F1-Score,Accuracy,MSE,R-Squared,A,0.3,Metrics,There is often an inverse relationship between Precision and Recall; F1-Score captures this trade-off.,F1-Score balances precision and recall. Consider the trade-off between these two metrics.
36,Dropout is a technique used for?,Optimization,Regularization,Data Augmentation,Feature Selection,B,0.4,Overfitting,Dropout randomly ignores some neurons during training to prevent co-adaptation and overfitting.,Think about the difference between how a model performs on data it has seen (training) versus new data (testing).
37,Naive Bayes assumes that features are?,Highly correlated,Independent,Continuous,Categorical only,B,0.5,Supervised,Naive Bayes makes the 'naive' assumption that all features are independent given the class label.,Supervised learning uses labeled examples to learn patterns. What does 'supervised' imply about the data?
38,What does the 'k' in k-NN (k-Nearest Neighbors) stand for?,Number of clusters,Number of neighbors to consider,Number of features,Number of classes,B,0.2,Supervised,k is the number of nearest neighbors to look at when voting for a class label.,Supervised learning uses labeled examples to learn patterns. What does 'supervised' imply about the data?
39,Which approach learns by interacting with an environment?,Supervised Learning,Unsupervised Learning,Reinforcement Learning,Transfer Learning,C,0.4,Theory,RL involves an agent taking actions in an environment to maximize cumulative reward.,Think about the fundamental concepts of Theory in machine learning.
40,What is the purpose of Data Normalization?,To convert text to numbers,To scale features to a similar range,To remove missing values,To reduce dimensionality,B,0.3,Preprocessing,"Normalization ensures all input features have a similar scale, helping Gradient Descent converge faster.","Normalization scales features to a similar range, helping optimization algorithms converge faster."
41,A high F1-score indicates?,High precision and low recall,Low precision and high recall,Balance between precision and recall,High accuracy only,C,0.5,Metrics,A high F1-score implies both high Precision and high Recall.,"When classes are imbalanced, accuracy can be misleading. Consider metrics that account for both precision and recall."
42,Which algorithm is not suitable for non-linear data without kernels?,Decision Trees,Random Forest,Linear Regression,K-NN,C,0.3,Supervised,Standard Linear Regression assumes a linear relationship and cannot fit complex non-linear data well.,Supervised learning uses labeled examples to learn patterns. What does 'supervised' imply about the data?
43,What is a Hyperparameter?,A parameter learned during training,A setting defined before training,A weight in the neural network,An evaluation metric,B,0.3,Theory,Hyperparameters (like learning rate or K) are set before learning begins.,"Hyperparameters are set before training begins, unlike model parameters which are learned during training."
44,Which step is NOT part of data preprocessing?,Normalization,Feature Selection,Weight Initialization,Handling Missing Values,C,0.4,Preprocessing,"Weight Initialization happens inside the model training process, not during data preprocessing.",Preprocessing prepares data for machine learning. Consider what Preprocessing does to the data.
45,Elbow Method is used to find optimal K in which algorithm?,KNN,K-Means,SVM,Decision Trees,B,0.4,Unsupervised,The Elbow Method plots variance against K to find the 'elbow' point where adding clusters yields diminishing returns.,Supervised learning uses labeled examples to learn patterns. What does 'supervised' imply about the data?
46,Which type of learning uses labeled data?,Supervised,Unsupervised,Reinforcement,Clustering,A,0.1,Supervised,Supervised learning relies on input-output pairs (labels) for training.,Supervised learning requires both input data and corresponding output labels to learn the mapping.
47,What does 'Epoch' mean in training?,One batch processed,One forward pass,One backward pass,The entire dataset passed forward and backward once,D,0.2,Optimization,One Epoch means the model has seen every example in the training dataset once.,An epoch means the model has seen every training example once. Think about what 'one complete pass' means.
48,Why do we use 'One-Hot Encoding'?,To normalize data,To convert categorical variables to numerical,To reduce overfitting,To initialize weights,B,0.3,Preprocessing,It converts categorical data into a format that ML algorithms can use efficiently.,One-Hot Encoding converts categorical variables into a binary format that ML algorithms can process.
49,Confusion Matrix is strictly used for?,Regression tasks,Classification tasks,Clustering tasks,Reinforcement Learning,B,0.2,Metrics,It is a table used to describe the performance of a classification model.,True Positive means the model correctly predicted the positive class. Think about what 'True' and 'Positive' mean.
50,Which algorithm uses a 'centroid' concept?,Decision Tree,Linear Regression,K-Means,Naive Bayes,C,0.3,Unsupervised,K-Means iteratively updates the position of cluster centroids.,Supervised learning uses labeled examples to learn patterns. What does 'supervised' imply about the data?
51,"In a neural network with batch normalization, what happens to the variance of activations during training?",Variance increases exponentially,Variance remains constant,Variance is normalized to approximately 1,Variance decreases linearly,C,0.65,Neural Networks,"Batch normalization normalizes activations by subtracting the mean and dividing by the standard deviation, effectively keeping variance around 1 and mean around 0.",Batch normalization standardizes activations. Think about what 'normalization' means for variance.
52,What is the primary advantage of using attention mechanisms in Transformers over RNNs?,Faster training speed,Better parallelization and handling of long-range dependencies,Lower memory requirements,Simpler architecture,B,0.7,Neural Networks,"Attention mechanisms allow parallel processing of all positions and can directly model long-range dependencies without sequential processing, unlike RNNs.",Attention mechanisms process all positions simultaneously. Consider the difference between sequential (RNN) and parallel processing.
53,"In gradient boosting, what is the purpose of the learning rate (shrinkage parameter)?",To increase model complexity,To prevent overfitting by scaling down each tree's contribution,To speed up convergence,To reduce computational cost,B,0.65,Supervised,"The learning rate (shrinkage) scales down each tree's contribution, making the model learn more slowly and reducing overfitting.","Shrinkage means making something smaller. In gradient boosting, it controls how much each new tree contributes."
54,What is the computational complexity of the forward pass in a Transformer with sequence length n and hidden dimension d?,O(n),O(n²d),O(nd),O(n²),B,0.75,Theory,"Self-attention computes attention scores for all pairs of positions (n² operations), each involving d-dimensional vectors, resulting in O(n²d) complexity.",Self-attention computes relationships between all position pairs. Count the operations: n positions × n positions × d dimensions.
55,"In variational autoencoders (VAE), what does the KL divergence term in the loss function enforce?",Reconstruction accuracy,Regularization of the latent space distribution,Discriminator training,Feature extraction,B,0.7,Unsupervised,"The KL divergence term ensures the learned latent distribution matches a prior (usually standard normal), regularizing the latent space.","KL divergence measures how different two distributions are. In VAE, it compares the learned latent distribution to a prior distribution."
56,What is the main difference between dropout and batch normalization in terms of when they are applied?,"Dropout is only for training, batch norm works in both training and inference",Both work identically in training and inference,"Batch norm is only for training, dropout works in both",They are applied at different layers,A,0.65,Neural Networks,Dropout randomly zeros neurons during training but is disabled during inference. Batch normalization uses different statistics (batch vs. running average) but is active in both modes.,Dropout randomly disables neurons. Think about when this behavior is needed (training vs. inference).
57,"In reinforcement learning, what problem does the credit assignment problem refer to?",Determining which actions led to rewards,Balancing exploration and exploitation,Handling sparse rewards,Computing Q-values,A,0.7,Theory,"The credit assignment problem is about determining which actions in a sequence were responsible for the final reward, especially when rewards are delayed.","Credit assignment means giving credit where it's due. In RL, this means identifying which actions caused the reward."
58,What is the key insight behind the Wasserstein GAN (WGAN) compared to standard GAN?,Using a different generator architecture,Replacing JS divergence with Wasserstein distance for more stable training,Adding more discriminator layers,Using different activation functions,B,0.75,Unsupervised,"WGAN uses Wasserstein distance instead of JS divergence, which provides smoother gradients and more stable training, especially when distributions don't overlap.",WGAN addresses training instability. The key change is in the distance metric used to measure how different distributions are.
59,"In meta-learning, what does MAML (Model-Agnostic Meta-Learning) optimize for?",Task-specific parameters,Initial parameters that can be quickly adapted to new tasks,Hyperparameters,Loss functions,B,0.8,Theory,"MAML finds initial parameters that, after a few gradient steps on a new task, perform well on that task. It optimizes for fast adaptation.",Meta-learning means learning to learn. MAML finds parameters that can quickly adapt (learn) to new tasks.
60,What is the primary reason for using residual connections in deep neural networks?,To reduce model size,To enable training of very deep networks by addressing vanishing gradients,To increase model capacity,To speed up inference,B,0.65,Neural Networks,"Residual connections create shortcuts that allow gradients to flow directly through, preventing vanishing gradients and enabling training of very deep networks.","Residual means 'remaining' or 'leftover'. Residual connections add the input to the output, creating a direct path for information flow."
61,"In contrastive learning, what is the purpose of negative samples?",To increase training data,To learn representations by contrasting similar and dissimilar examples,To reduce overfitting,To balance classes,B,0.7,Unsupervised,"Negative samples (dissimilar examples) are used to push apart representations of different classes, while positive samples pull similar examples together.",Contrastive means comparing differences. Negative samples help the model learn what NOT to group together.
62,What does the reparameterization trick in VAEs enable?,Faster inference,Backpropagation through stochastic sampling,Better reconstruction,Reduced memory usage,B,0.75,Unsupervised,"The reparameterization trick separates the stochastic part (sampling) from the deterministic part, allowing gradients to flow through the sampling operation via backpropagation.","Reparameterization means rewriting something in a different form. In VAE, it separates randomness from learnable parameters."
63,"In federated learning, what is the main challenge addressed by differential privacy?",Communication efficiency,Privacy preservation while maintaining model utility,Model convergence,Computational cost,B,0.8,Theory,Differential privacy adds noise to protect individual data privacy while still allowing the model to learn useful patterns from aggregated updates.,Differential privacy protects individual privacy. Think about how to learn from data without revealing information about specific individuals.
64,What is the key difference between hard attention and soft attention mechanisms?,Hard attention is faster,"Hard attention selects discrete positions, soft attention uses weighted combinations",Soft attention is more accurate,They use different activation functions,B,0.7,Neural Networks,"Hard attention selects specific positions (discrete), while soft attention computes weighted combinations of all positions (continuous and differentiable).","Hard means discrete/categorical, soft means continuous/probabilistic. Think about how each selects which positions to focus on."
65,"In knowledge distillation, what is the role of temperature scaling?",To speed up training,To soften the probability distribution for better knowledge transfer,To reduce model size,To increase accuracy,B,0.75,Theory,"Temperature scaling softens the teacher model's probability distribution, making it easier for the student to learn the relative relationships between classes.","Temperature in probability distributions makes them smoother (higher temp) or sharper (lower temp). In distillation, we want smoother distributions."
66,What is the computational advantage of using separable convolutions in depthwise separable convolution?,Better feature extraction,Reduced parameters and operations compared to standard convolution,Higher accuracy,Simpler implementation,B,0.7,Neural Networks,"Depthwise separable convolution splits standard convolution into depthwise and pointwise operations, significantly reducing computational cost and parameters.",Separable means breaking something into parts. Separable convolution splits the operation to reduce computation.
67,"In few-shot learning, what does the support set represent?",Test examples,Training examples for a new task,Validation examples,Pre-training data,B,0.75,Theory,"In few-shot learning, the support set contains the few labeled examples provided for a new task, used to adapt the model quickly.","Support means help or foundation. In few-shot learning, the support set provides the few examples needed to learn a new task."
68,What is the primary purpose of the discriminator in a GAN?,To generate samples,"To distinguish between real and fake samples, providing training signal",To encode data,To reduce dimensionality,B,0.65,Unsupervised,"The discriminator acts as a critic, learning to distinguish real from generated samples, which provides the adversarial signal to train the generator.",Discriminate means to tell apart. The discriminator learns to tell real and fake samples apart.
69,"In neural architecture search (NAS), what is the main challenge of the search space?",Computational cost,The exponential size of possible architectures,Lack of training data,Overfitting,B,0.8,Theory,"The search space of possible neural architectures is exponentially large, making exhaustive search infeasible and requiring efficient search strategies.","Search space means all possible options. In NAS, there are countless ways to design a network, making the space huge."
70,What is the key innovation of the Transformer architecture compared to RNNs?,Using attention instead of recurrence,Using convolutional layers,Using dropout,Using batch normalization,A,0.7,Neural Networks,"Transformers replace recurrence (sequential processing) with self-attention mechanisms, enabling parallel processing and better handling of long-range dependencies.",Transformers don't process sequences step-by-step like RNNs. They use attention to process all positions at once.
71,"In reinforcement learning, what does the exploration-exploitation trade-off refer to?",Choosing between different algorithms,Balancing trying new actions vs. using known good actions,Selecting hyperparameters,Training vs. inference,B,0.65,Theory,"Exploration means trying new actions to discover potentially better strategies, while exploitation means using known good actions. The trade-off is fundamental to RL.","Exploration is about discovering new things, exploitation is about using what you know works. RL needs to balance both."
72,What is the primary advantage of using group normalization over batch normalization?,Better accuracy,Stable performance with small batch sizes,Faster training,Lower memory usage,B,0.75,Neural Networks,"Group normalization normalizes across groups of channels rather than batches, making it independent of batch size and stable even with batch size 1.",Group normalization doesn't depend on batch size. Think about when batch size is small or variable.
73,"In contrastive learning with SimCLR, what is the role of data augmentation?",To increase dataset size,To create positive pairs for contrastive learning,To reduce overfitting,To balance classes,B,0.8,Unsupervised,"Data augmentation creates different views of the same image, which serve as positive pairs that should have similar representations in contrastive learning.","In SimCLR, augmented versions of the same image should be similar. Think about what makes a positive pair."
74,What is the computational complexity of training a standard RNN for a sequence of length n?,O(n),O(n²),O(n log n),O(1),A,0.65,Theory,"RNNs process sequences sequentially, one time step at a time, resulting in O(n) time complexity for a sequence of length n.","RNNs process sequences step by step. Each step takes constant time, so n steps take O(n) time."
75,"In transfer learning, what is fine-tuning?",Training from scratch,Adjusting pre-trained model parameters on a new task,Selecting hyperparameters,Data preprocessing,B,0.7,Theory,"Fine-tuning involves taking a pre-trained model and continuing training (adjusting weights) on a new, typically related task with a smaller learning rate.","Fine-tuning means making small adjustments. In transfer learning, you start with a pre-trained model and make small adjustments for a new task."
76,What is the key difference between supervised and self-supervised learning?,Self-supervised uses more data,Self-supervised generates labels from the data itself,Supervised is faster,They use different architectures,B,0.75,Theory,"Self-supervised learning creates supervisory signals from the data structure itself (e.g., predicting masked words, next frame), without human-annotated labels.","Self-supervised means the supervision comes from the data itself, not from external labels. Think about how labels are created."
77,"In neural machine translation, what problem does the attention mechanism solve?",Overfitting,Information bottleneck in encoder-decoder architectures,Vanishing gradients,Slow training,B,0.8,Neural Networks,"Attention allows the decoder to directly access all encoder hidden states, solving the bottleneck problem where all information was compressed into a single context vector.","Without attention, all information must pass through a single bottleneck. Attention provides direct access to all information."
78,"What is the primary purpose of the query, key, and value vectors in self-attention?",To reduce parameters,To compute attention weights and weighted feature representations,To speed up computation,To prevent overfitting,B,0.85,Neural Networks,"Query and key compute attention weights (how much to attend), while value contains the actual information to be aggregated based on those weights.","Query asks 'what am I looking for?', key answers 'what do I have?', value is 'what information do I provide?'"
79,"In adversarial training, what is the goal of adding adversarial examples to training?",To increase accuracy,To improve robustness against adversarial attacks,To speed up training,To reduce model size,B,0.75,Neural Networks,"Adversarial training exposes the model to adversarial examples during training, teaching it to be robust against such attacks.",Adversarial examples are designed to fool the model. Training on them helps the model become resistant to such attacks.
80,What is the key insight behind the success of residual networks (ResNets)?,Using more layers,Learning residual mappings makes optimization easier,Using better activation functions,Reducing parameters,B,0.7,Neural Networks,"ResNets learn residual (difference) mappings rather than direct mappings, making it easier to learn identity functions and enabling training of very deep networks.","Residual means the difference or remainder. ResNets learn what to add to the input, not what the output should be directly."
81,"In graph neural networks (GNNs), what does message passing refer to?",Sending data between layers,Aggregating information from neighboring nodes,Training communication,Data preprocessing,B,0.8,Neural Networks,"Message passing in GNNs involves nodes aggregating information from their neighbors, allowing each node to incorporate structural and feature information from its local graph neighborhood.","In graphs, nodes are connected. Message passing means nodes share information with their neighbors."
82,What is the primary challenge in training GANs that WGAN addresses?,Slow convergence,Training instability and mode collapse,High memory usage,Overfitting,B,0.75,Unsupervised,Standard GANs suffer from training instability and mode collapse. WGAN uses Wasserstein distance which provides more stable gradients and better training dynamics.,GANs are notoriously difficult to train. WGAN specifically addresses the instability and mode collapse problems.
83,"In few-shot learning with prototypical networks, what does a prototype represent?",A single example,The mean embedding of support examples for a class,A hyperparameter,A loss function,B,0.8,Theory,"A prototype is the average (mean) of the embedded support examples for each class, representing the class center in the embedding space.","Prototype means a typical example. In prototypical networks, it's the average representation of all examples in a class."
84,What is the key difference between in-context learning and fine-tuning in large language models?,"In-context learning updates weights, fine-tuning doesn't",In-context learning uses examples in the prompt without weight updates,Fine-tuning is faster,They use different architectures,B,0.85,Theory,"In-context learning provides examples in the input prompt and the model learns from them without updating weights, while fine-tuning actually modifies model parameters.",In-context means within the context (prompt). In-context learning uses examples in the prompt without changing the model.
85,"In neural architecture search, what is the difference between one-shot and multi-shot NAS?","One-shot trains one architecture, multi-shot trains multiple","One-shot uses a supernet, multi-shot trains each architecture separately",Multi-shot is faster,They use different search spaces,B,0.9,Theory,"One-shot NAS trains a supernet (overparameterized network) once and evaluates sub-networks by sharing weights, while multi-shot trains each architecture from scratch.","One-shot means doing it once. In NAS, one-shot trains a supernet once and evaluates many architectures from it."
86,What is the primary advantage of using layer normalization over batch normalization in Transformers?,Better accuracy,Sequence-length independent normalization,Faster training,Lower memory,B,0.75,Neural Networks,"Layer normalization normalizes across features for each sample independently, making it work consistently regardless of batch size or sequence length.","Layer norm normalizes within each sample, not across samples. Think about why this matters for sequences of different lengths."
87,"In reinforcement learning, what does the policy gradient theorem enable?",Value function estimation,Direct gradient-based optimization of policy parameters,Q-learning,Exploration strategies,B,0.8,Theory,"The policy gradient theorem provides a way to compute gradients of the expected return with respect to policy parameters, enabling direct policy optimization.",Policy gradient means optimizing the policy directly using gradients. The theorem shows how to compute these gradients.
88,What is the key innovation of BERT's masked language modeling objective?,Using bidirectional context,Predicting masked tokens using both left and right context,Using attention,Using transformers,B,0.8,NLP,"BERT's masked language modeling allows the model to use both left and right context to predict masked tokens, enabling true bidirectional understanding unlike previous left-to-right models.",BERT can look at both sides of a masked word. Previous models could only look left (or right).
89,"In contrastive learning, what is the temperature parameter's role in the contrastive loss?",To control learning rate,"To scale the similarity scores, controlling how hard the contrastive task is",To balance positive and negative pairs,To reduce overfitting,B,0.75,Unsupervised,"Temperature scales the similarity scores in the softmax, with lower temperatures making the distribution sharper (harder negatives) and higher temperatures making it softer.",Temperature in probability distributions controls sharpness. Lower temperature = sharper = harder to distinguish.
90,What is the computational advantage of using flash attention in Transformers?,Better accuracy,Reduced memory usage and faster computation by avoiding materializing full attention matrix,Simpler implementation,Lower parameters,B,0.9,Neural Networks,"Flash attention computes attention in blocks without storing the full attention matrix, reducing memory from O(n²) to O(n) and enabling longer sequences.",Flash attention avoids storing the full attention matrix. Think about the memory savings for long sequences.
